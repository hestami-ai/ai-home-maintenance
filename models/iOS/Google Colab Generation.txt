List contents directory
!ls -lh /content





YoloV11 Nano to CoreML Export on Google Colab:

# 1. Install the library
!pip install ultralytics

# 2. Export the model
from ultralytics import YOLO

# Load the model (it downloads automatically)
model = YOLO('yolo11n.pt') 

# Export to CoreML with NMS enabled
model.export(format='coreml', nms=True)
!zip -r yolo_model.zip /content/yolo11n.mlpackage
files.download("yolo_model.zip")



MobileSAM to CoreML Export on Google Colab:

# @title üöÄ MobileSAM to CoreML Export Script
# This script exports the MobileSAM Encoder and Decoder to Apple CoreML format.

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from google.colab import files

# --- 1. SETUP & INSTALLATION ---
print("üì¶ Installing dependencies...")
!pip install -q coremltools
!pip install -q git+https://github.com/ChaoningZhang/MobileSAM.git

import coremltools as ct
from mobile_sam import sam_model_registry

# Download weights
if not os.path.exists("mobile_sam.pt"):
    print("‚¨áÔ∏è Downloading MobileSAM weights...")
    !wget -q https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt

# --- 2. MODEL LOADING ---
print("üß† Loading PyTorch Model...")
model_type = "vit_t"
checkpoint = "mobile_sam.pt"
device = "cpu" # CoreML export happens on CPU
mobile_sam = sam_model_registry[model_type](checkpoint=checkpoint)
mobile_sam.to(device)
mobile_sam.eval()

# --- 3. EXPORT ENCODER (The Heavy Lifter) ---
print("üèóÔ∏è Exporting Image Encoder...")

class EncoderWrapper(nn.Module):
    def __init__(self, sam_model):
        super().__init__()
        self.model = sam_model.image_encoder

    def forward(self, x):
        # MobileSAM expects input to be 1024x1024
        return self.model(x)

# Trace the encoder
# Input: 1 batch, 3 channels, 1024 height, 1024 width
example_input_image = torch.rand(1, 3, 1024, 1024)
traced_encoder = torch.jit.trace(EncoderWrapper(mobile_sam), example_input_image)

# Convert to CoreML
encoder_mlmodel = ct.convert(
    traced_encoder,
    inputs=[ct.ImageType(name="image", shape=example_input_image.shape, scale=1/255.0)],
    outputs=[ct.TensorType(name="image_embeddings")],
    minimum_deployment_target=ct.target.iOS16,
    compute_units=ct.ComputeUnit.ALL
)
encoder_mlmodel.save("MobileSAM_Encoder.mlpackage")
print("‚úÖ Encoder Exported!")

# --- 4. EXPORT DECODER (The Precision Tool) ---
print("üèóÔ∏è Exporting Mask Decoder...")

# We need a wrapper to simplify the complex SAM inputs for CoreML
class DecoderWrapper(nn.Module):
    def __init__(self, sam_model):
        super().__init__()
        self.mask_decoder = sam_model.mask_decoder
        self.prompt_encoder = sam_model.prompt_encoder

    def forward(self, image_embeddings, point_coords, point_labels):
        # 1. Embed Points (We skip boxes/masks for simplicity in this V1 export)
        # Note: CoreML handles fixed inputs best. We assume a max of 5 points for "Zero-Touch" usage.
        sparse_embeddings, dense_embeddings = self.prompt_encoder(
            points=(point_coords, point_labels),
            boxes=None,
            masks=None,
        )

        # 2. Predict Masks
        low_res_masks, iou_predictions = self.mask_decoder(
            image_embeddings=image_embeddings,
            image_pe=self.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False, # We only want the best mask
        )
        return low_res_masks, iou_predictions

# Trace the decoder
# Inputs for tracing:
dummy_embed = torch.randn(1, 256, 64, 64) # Output from Encoder
dummy_coords = torch.randint(0, 1024, (1, 5, 2)).float() # 5 Points (X,Y)
dummy_labels = torch.randint(0, 1, (1, 5)).float() # 5 Labels

decoder_wrapper = DecoderWrapper(mobile_sam)
decoder_wrapper.eval()
traced_decoder = torch.jit.trace(decoder_wrapper, (dummy_embed, dummy_coords, dummy_labels))

# Convert to CoreML
decoder_mlmodel = ct.convert(
    traced_decoder,
    inputs=[
        ct.TensorType(name="image_embeddings", shape=dummy_embed.shape),
        ct.TensorType(name="point_coords", shape=dummy_coords.shape),
        ct.TensorType(name="point_labels", shape=dummy_labels.shape)
    ],
    outputs=[
        ct.TensorType(name="low_res_masks"),
        ct.TensorType(name="iou_predictions")
    ],
    minimum_deployment_target=ct.target.iOS16
)
decoder_mlmodel.save("MobileSAM_Decoder.mlpackage")
print("‚úÖ Decoder Exported!")

# --- 5. ZIP AND DOWNLOAD ---
print("üì¶ Zipping and Downloading...")
!zip -r MobileSAM_CoreML_Export.zip MobileSAM_Encoder.mlpackage MobileSAM_Decoder.mlpackage
files.download("MobileSAM_CoreML_Export.zip")